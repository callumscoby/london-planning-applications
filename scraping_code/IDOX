## This is an example that scrapes applications from the City of London planning portal between 10-02-2022 and 11-02-2022.

# Function to calculate the dates in the given range according to the step interval chosen (1 in this example)
def date_range(start_date, end_date):
    for d in range(0, int((end_date - start_date).days) + 1, 1): # Modify the step interval
        yield start_date + timedelta(d)

date_idox = []

# Defining the date range
start_date = date(2022, 2, 10)
end_date = date(2022, 2, 11)

# Applying the function and appending the resulting dates to a list
for dt in date_range(start_date, end_date):
    dt = dt.strftime('%d/%m/%Y')
    date_idox.append(dt)

print(date_idox)

description_text = []
address_text = []
meta_text = []
decision_text = []
link_numbers = []

# Setting the browser to headless mode- this reduces runtime and memory usage
chrome_options = Options()
chrome_options.add_argument("--headless")

# Configuring the driver and setting a default wait time of 10 seconds for an element to become clickable
driver = webdriver.Chrome()
wait = WebDriverWait(driver, 10)

# Enter relevant url
url = 'https://www.planning2.cityoflondon.gov.uk/online-applications/search.do?action=advanced'

for i in range(len(date_idox)):
    while (i + 1) < len(date_idox):
        driver.get(url) # Starting the Chromedriver and getting the URL
        print(f"""
-------------------------------------
Getting url: {url} for date range: {i}""")
        time.sleep(20) # Once URL has been reached, wait 20 seconds

        # Inputting start and end dates into their respective fields
        try:
            start_input = driver.find_element(By.XPATH, '//input[@id="applicationReceivedStart"]')
            start_input.send_keys(date_idox[i])

            end_input = driver.find_element(By.XPATH, '//input[@id="applicationReceivedEnd"]')
            end_input.send_keys(date_idox[i + 1])
        except:
            start_input = driver.find_element(By.XPATH, '//input[@id="applicationValidatedStart"]')
            start_input.send_keys(date_idox[i])

            end_input = driver.find_element(By.XPATH, '//input[@id="applicationValidatedEnd"]')
            end_input.send_keys(date_idox[i + 1])

        # Wait until the 'Search' button is clickable, then click it and wait for 5 seconds
        try:
            wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@class="button primary recaptcha-submit"]')))
            search_button = driver.find_element(By.XPATH, '//input[@class="button primary recaptcha-submit"]')
            search_button.click()
        except:
            wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@value="Search"]')))
            search_button = driver.find_element(By.XPATH, '//input[@value="Search"]')
            search_button.click()
            time.sleep(5)

        # If multiple applications are returned:
        if driver.find_elements(By.XPATH, '//*[@id="resultsPerPage"]'):

            # Click the 'Results per page' dropdown and choose to display 100 results
            dropdown_results_pp = Select(driver.find_element(By.XPATH, '//*[@id="resultsPerPage"]'))
            dropdown_results_pp.select_by_visible_text('100')

            # Wait until the 'Go' button is clickable, then click it to display 100 results and wait 5 seconds
            wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@type="submit"]')))
            go_button = driver.find_element(By.XPATH, '//input[@type="submit"]')
            go_button.click()
            time.sleep(5)

            # Find all <a> elements (links) on the page
            page_links = driver.find_elements(By.XPATH, '//ul[@id="searchresults"]//a')

            # Creating a variable of the number of links present on the page
            no_page_links = len(page_links)

            # Iterating through the links on the page
            try:
                for link in range(no_page_links):

                    # Storing the current link number for use in dealing with errors
                    link_numbers.append(link)
                    print(f"Link {link+1} of {no_page_links}")

                    # Wait until the link is clickable, then click it and wait 10 seconds for page loading
                    wait.until(EC.element_to_be_clickable((By.XPATH,'//ul[@id="searchresults"]//a')))
                    link = driver.find_elements(By.XPATH, '//ul[@id="searchresults"]//a')[link]
                    link.click()
                    time.sleep(10)

                    # Identify all <th> elements (row names) on the page
                    th_text = []
                    ths = driver.find_elements(By.XPATH, '//tbody//tr/th')  # Finding all 'th' elements
                    th_text.append([th.text for th in ths])  # Converting all 'th' elements to their text string
                    th_text = [item for sublist in th_text for item in sublist]  # Flattening the list

                    # Identifying the position of each relevant <th> element in the list
                    meta_index = int(th_text.index("Reference"))
                    add_index = int(th_text.index("Address"))
                    prop_index = int(th_text.index("Proposal"))

                    # Using the positions of the <th> elements to access the relevant
                    # <td> elements on the page
                    desc = driver.find_elements(By.XPATH, (f'//tbody//tr[{prop_index + 1}]//td'))
                    address = driver.find_elements(By.XPATH, (f'//tbody//tr[{add_index + 1}]//td'))
                    meta = driver.find_elements(By.XPATH, (f'//tbody//tr[{meta_index + 1}]//td'))

                    # Extracting the text of each relevant <td> element
                    description_text.append([d.get_attribute("innerHTML").strip() for d in desc])
                    address_text.append([a.text for a in address])
                    meta_text.append([m.text for m in meta])

                    # If a 'Decision' row is on the page:
                    if driver.find_elements(By.XPATH, '//th[contains(text(), "Decision")]'):
                        try:
                            # Identify the position of the 'Decision' <th> element
                            deci_index = int(th_text.index("Decision"))
                            # Use this position to find the corresponding <td> element
                            decision = driver.find_elements(By.XPATH, (f'//tbody//tr[{deci_index + 1}]//td'))
                            # Extracting the text of the 'Decision' <td> element
                            decision_text.append([ds.text for ds in decision])
                        except:
                            # If no 'Decision' is found:
                            print('No decision made')
                            decision_text.append(['No decision made'])

                    # Once complete, wait for the 'Back to search results' button to become clickable,
                    # then click it and wait 30 seconds
                    wait.until(EC.element_to_be_clickable((By.XPATH, '//div[@id="applicationTools"]//a[@id="searchresultsback"]')))
                    back_to_search_btn = driver.find_element(By.XPATH, '//div[@id="applicationTools"]//a[@id="searchresultsback"]')
                    back_to_search_btn.click()
                    time.sleep(30)

                break # Move on to the next date range if the above is successfully completed

            # If a NoSuchElementException arises (this usually occurs when the page is slow to load), attempt
            # to repeat the entire above process:
            except NoSuchElementException:
                print('TimeOut- trying again')
                driver.get(url) # Starting the Chromedriver and getting the URL
                time.sleep(20) # Once URL has been reached, wait 20 seconds

                # Inputting start and end dates into their respective fields
                try:
                    start_input = driver.find_element(By.XPATH, '//input[@id="applicationReceivedStart"]')
                    start_input.send_keys(date_idox[i])

                    end_input = driver.find_element(By.XPATH, '//input[@id="applicationReceivedEnd"]')
                    end_input.send_keys(date_idox[i + 1])
                except:
                    start_input = driver.find_element(By.XPATH, '//input[@id="applicationValidatedStart"]')
                    start_input.send_keys(date_idox[i])

                    end_input = driver.find_element(By.XPATH, '//input[@id="applicationValidatedEnd"]')
                    end_input.send_keys(date_idox[i + 1])

                # Wait until the 'Search' button is clickable, then click it and wait for 5 seconds
                try:
                    wait.until(EC.element_to_be_clickable(
                        (By.XPATH, '//input[@class="button primary recaptcha-submit"]')))
                    search_button = driver.find_element(By.XPATH,
                                                        '//input[@class="button primary recaptcha-submit"]')
                    search_button.click()
                except:
                    wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@value="Search"]')))
                    search_button = driver.find_element(By.XPATH, '//input[@value="Search"]')
                    search_button.click()
                    time.sleep(5)

                # If multiple applications are returned:
                if driver.find_elements(By.XPATH, '//*[@id="resultsPerPage"]'):

                    # Click the 'Results per page' dropdown and choose to display 100 results
                    dropdown_results_pp = Select(driver.find_element(By.XPATH, '//*[@id="resultsPerPage"]'))
                    dropdown_results_pp.select_by_visible_text('100')

                    # Wait until the 'Go' button is clickable, then click it to display 100 results and wait 5 seconds
                    wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@type="submit"]')))
                    go_button = driver.find_element(By.XPATH, '//input[@type="submit"]')
                    time.sleep(10)
                    go_button.click()
                    time.sleep(10)

                    # Iterating through the links on the page
                    try:
                        # Identifying the last link clicked before the website crash
                        for relevant_link in range(link_numbers[-1], no_page_links):
                            link_numbers.append(relevant_link)
                            print(f'Relevant link number: {relevant_link+1}')

                            # Waiting until links on the page are clickable
                            wait.until(EC.element_to_be_clickable((By.XPATH, '//ul[@id="searchresults"]//a')))

                            # Clicking on the link that was clicked before the crash and waiting 10 seconds
                            link = driver.find_elements(By.XPATH, '//ul[@id="searchresults"]//a')[relevant_link]
                            link.click()
                            time.sleep(10)

                            # Identify all <th> elements (row names) on the page
                            th_text = []
                            ths = driver.find_elements(By.XPATH, '//tbody//tr/th')  # Finding all 'th' elements
                            th_text.append([th.text for th in ths])  # Converting all 'th' elements to their text string
                            th_text = [item for sublist in th_text for item in sublist]  # Flattening the list

                            # Identifying the position of each relevant <th> element in the list
                            meta_index = int(th_text.index("Reference"))
                            add_index = int(th_text.index("Address"))
                            prop_index = int(th_text.index("Proposal"))

                            # Using the positions of the <th> elements to access the relevant
                            # <td> elements on the page
                            desc = driver.find_elements(By.XPATH, (f'//tbody//tr[{prop_index + 1}]//td'))
                            address = driver.find_elements(By.XPATH, (f'//tbody//tr[{add_index + 1}]//td'))
                            meta = driver.find_elements(By.XPATH, (f'//tbody//tr[{meta_index + 1}]//td'))

                            # Extracting the text of each relevant <td> element
                            description_text.append([d.get_attribute("innerHTML").strip() for d in desc])
                            address_text.append([a.text for a in address])
                            meta_text.append([m.text for m in meta])

                            # If a 'Decision' row is on the page:
                            if driver.find_elements(By.XPATH, '//th[contains(text(), "Decision")]'):
                                try:
                                    # Identify the position of the 'Decision' <th> element
                                    deci_index = int(th_text.index("Decision"))
                                    # Use this position to find the corresponding <td> element
                                    decision = driver.find_elements(By.XPATH,(f'//tbody//tr[{deci_index + 1}]//td'))
                                    # Extracting the text of the 'Decision' <td> element
                                    decision_text.append([ds.text for ds in decision])
                                except:
                                    # If no 'Decision' is found:
                                    decision_text.append(['No decision made'])

                            # Once complete, wait for the 'Back to search results' button to become clickable,
                            # then click it and wait 30 seconds
                            wait.until(EC.element_to_be_clickable((By.XPATH, '//div[@id="applicationTools"]//a[@id="searchresultsback"]')))
                            back_to_search_btn = driver.find_element(By.XPATH, '//div[@id="applicationTools"]//a[@id="searchresultsback"]')
                            back_to_search_btn.click()
                            time.sleep(30)

                        break # Move on to the next date range if the above is successfully completed

                    except: # If this has not worked, return to the beginning of the whole code block
                        print('Website completely crashed- starting from the beginning')

                # If there is only one application on the page:
                elif driver.find_elements(By.XPATH, '//table[@id="simpleDetailsTable"]'):
                    print('Only one result found')

                    # Identify all <th> elements (row names) on the page
                    th_text = []
                    ths = driver.find_elements(By.XPATH, '//tbody//tr/th')  # Finding all 'th' elements
                    th_text.append([th.text for th in ths])  # Converting all 'th' elements to their text string
                    th_text = [item for sublist in th_text for item in sublist]  # Flattening the list

                    # Identifying the position of each relevant <th> element in the list
                    meta_index = int(th_text.index("Reference"))
                    add_index = int(th_text.index("Address"))
                    prop_index = int(th_text.index("Proposal"))

                    # Using the positions of the <th> elements to access the relevant
                    # <td> elements on the page
                    desc = driver.find_elements(By.XPATH, (f'//tbody//tr[{prop_index + 1}]//td'))
                    address = driver.find_elements(By.XPATH, (f'//tbody//tr[{add_index + 1}]//td'))
                    meta = driver.find_elements(By.XPATH, (f'//tbody//tr[{meta_index + 1}]//td'))

                    # Extracting the text of each relevant <td> element
                    description_text.append([d.get_attribute("innerHTML").strip() for d in desc])
                    address_text.append([a.text for a in address])
                    meta_text.append([m.text for m in meta])

                    # If a 'Decision' row is on the page:
                    if driver.find_elements(By.XPATH, '//th[contains(text(), "Decision")]'):
                        try:
                            # Identify the position of the 'Decision' <th> element
                            deci_index = int(th_text.index("Decision"))
                            # Use this position to find the corresponding <td> element
                            decision = driver.find_elements(By.XPATH, (f'//tbody//tr[{deci_index + 1}]//td'))
                            # Extracting the text of the 'Decision' <td> element
                            decision_text.append([ds.text for ds in decision])
                        except:
                            # If no 'Decision' is found:
                            decision_text.append(['No decision made'])
                    break # Move on to the next date range if the above is successfully completed

                # If there are no applications on the page:
                elif driver.find_element(By.XPATH, '//div[@class="messagebox"]'):
                    print('No results for this date range')
                    break # Move on to the next date range
                else:
                    break

            # If there are any other errors, repeat the above process
            except:
                print('Website crashed- trying to carry on')
                driver.get(url) # Starting the Chromedriver and getting the URL
                time.sleep(20) # Once URL has been reached, wait 20 seconds

                # Inputting start and end dates into their respective fields
                try:
                    start_input = driver.find_element(By.XPATH, '//input[@id="applicationReceivedStart"]')
                    start_input.send_keys(date_idox[i])

                    end_input = driver.find_element(By.XPATH, '//input[@id="applicationReceivedEnd"]')
                    end_input.send_keys(date_idox[i + 1])
                except:
                    start_input = driver.find_element(By.XPATH, '//input[@id="applicationValidatedStart"]')
                    start_input.send_keys(date_idox[i])

                    end_input = driver.find_element(By.XPATH, '//input[@id="applicationValidatedEnd"]')
                    end_input.send_keys(date_idox[i + 1])

                # Wait until the 'Search' button is clickable, then click it and wait for 5 seconds
                try:
                    wait.until(EC.element_to_be_clickable(
                        (By.XPATH, '//input[@class="button primary recaptcha-submit"]')))
                    search_button = driver.find_element(By.XPATH,
                                                        '//input[@class="button primary recaptcha-submit"]')
                    search_button.click()
                except:
                    wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@value="Search"]')))
                    search_button = driver.find_element(By.XPATH, '//input[@value="Search"]')
                    search_button.click()
                    time.sleep(5)

                # If multiple applications are returned:
                if driver.find_elements(By.XPATH, '//*[@id="resultsPerPage"]'):

                    # Click the 'Results per page' dropdown and choose to display 100 results
                    dropdown_results_pp = Select(driver.find_element(By.XPATH, '//*[@id="resultsPerPage"]'))
                    dropdown_results_pp.select_by_visible_text('100')

                    # Wait until the 'Go' button is clickable, then click it to display 100 results and wait 5 seconds
                    wait.until(EC.element_to_be_clickable((By.XPATH, '//input[@type="submit"]')))
                    go_button = driver.find_element(By.XPATH, '//input[@type="submit"]')
                    time.sleep(10)
                    go_button.click()
                    time.sleep(10)

                    # Iterating through the links on the page
                    try:
                        # Identifying the last link clicked before the website crash
                        for relevant_link in range(link_numbers[-1], no_page_links):
                            link_numbers.append(relevant_link)
                            print(f'Relevant link number: {relevant_link+1}')

                            # Waiting until links on the page are clickable
                            wait.until(EC.element_to_be_clickable((By.XPATH, '//ul[@id="searchresults"]//a')))

                            # Clicking on the link that was clicked before the crash and waiting 10 seconds
                            link = driver.find_elements(By.XPATH, '//ul[@id="searchresults"]//a')[relevant_link]
                            link.click()
                            time.sleep(10)

                            # Identify all <th> elements (row names) on the page
                            th_text = []
                            ths = driver.find_elements(By.XPATH, '//tbody//tr/th')  # Finding all 'th' elements
                            th_text.append([th.text for th in ths])  # Converting all 'th' elements to their text string
                            th_text = [item for sublist in th_text for item in sublist]  # Flattening the list

                            # Identifying the position of each relevant <th> element in the list
                            meta_index = int(th_text.index("Reference"))
                            add_index = int(th_text.index("Address"))
                            prop_index = int(th_text.index("Proposal"))

                            # Using the positions of the <th> elements to access the relevant
                            # <td> elements on the page
                            desc = driver.find_elements(By.XPATH, (f'//tbody//tr[{prop_index + 1}]//td'))
                            address = driver.find_elements(By.XPATH, (f'//tbody//tr[{add_index + 1}]//td'))
                            meta = driver.find_elements(By.XPATH, (f'//tbody//tr[{meta_index + 1}]//td'))

                            # Extracting the text of each relevant <td> element
                            description_text.append([d.get_attribute("innerHTML").strip() for d in desc])
                            address_text.append([a.text for a in address])
                            meta_text.append([m.text for m in meta])

                            # If a 'Decision' row is on the page:
                            if driver.find_elements(By.XPATH, '//th[contains(text(), "Decision")]'):
                                try:
                                    # Identify the position of the 'Decision' <th> element
                                    deci_index = int(th_text.index("Decision"))
                                    # Use this position to find the corresponding <td> element
                                    decision = driver.find_elements(By.XPATH,(f'//tbody//tr[{deci_index + 1}]//td'))
                                    # Extracting the text of the 'Decision' <td> element
                                    decision_text.append([ds.text for ds in decision])
                                except:
                                    # If no 'Decision' is found:
                                    decision_text.append(['No decision made'])

                            # Once complete, wait for the 'Back to search results' button to become clickable,
                            # then click it and wait 30 seconds
                            wait.until(EC.element_to_be_clickable((By.XPATH, '//div[@id="applicationTools"]//a[@id="searchresultsback"]')))
                            back_to_search_btn = driver.find_element(By.XPATH, '//div[@id="applicationTools"]//a[@id="searchresultsback"]')
                            back_to_search_btn.click()
                            time.sleep(30)

                        break # Move on to the next date range if the above is successfully completed

                    except: # If this has not worked, return to the beginning of the whole code block
                        print('Website completely crashed- starting from the beginning')

                # If there is only one application on the page:
                elif driver.find_elements(By.XPATH, '//table[@id="simpleDetailsTable"]'):
                    print('Only one result found')

                    # Identify all <th> elements (row names) on the page
                    th_text = []
                    ths = driver.find_elements(By.XPATH, '//tbody//tr/th')  # Finding all 'th' elements
                    th_text.append([th.text for th in ths])  # Converting all 'th' elements to their text string
                    th_text = [item for sublist in th_text for item in sublist]  # Flattening the list

                    # Identifying the position of each relevant <th> element in the list
                    meta_index = int(th_text.index("Reference"))
                    add_index = int(th_text.index("Address"))
                    prop_index = int(th_text.index("Proposal"))

                    # Using the positions of the <th> elements to access the relevant
                    # <td> elements on the page
                    desc = driver.find_elements(By.XPATH, (f'//tbody//tr[{prop_index + 1}]//td'))
                    address = driver.find_elements(By.XPATH, (f'//tbody//tr[{add_index + 1}]//td'))
                    meta = driver.find_elements(By.XPATH, (f'//tbody//tr[{meta_index + 1}]//td'))

                    # Extracting the text of each relevant <td> element
                    description_text.append([d.get_attribute("innerHTML").strip() for d in desc])
                    address_text.append([a.text for a in address])
                    meta_text.append([m.text for m in meta])

                    # If a 'Decision' row is on the page:
                    if driver.find_elements(By.XPATH, '//th[contains(text(), "Decision")]'):
                        try:
                            # Identify the position of the 'Decision' <th> element
                            deci_index = int(th_text.index("Decision"))
                            # Use this position to find the corresponding <td> element
                            decision = driver.find_elements(By.XPATH, (f'//tbody//tr[{deci_index + 1}]//td'))
                            # Extracting the text of the 'Decision' <td> element
                            decision_text.append([ds.text for ds in decision])
                        except:
                            # If no 'Decision' is found:
                            decision_text.append(['No decision made'])
                    break # Move on to the next date range if the above is successfully completed

                # If there are no applications on the page:
                elif driver.find_element(By.XPATH, '//div[@class="messagebox"]'):
                    print('No results for this date range')
                    break # Move on to the next date range
                else:
                    break

        # If there is only one application on the page:
        elif driver.find_elements(By.XPATH, '//table[@id="simpleDetailsTable"]'):
            print('Only one result found')

            # Identify all <th> elements (row names) on the page
            th_text = []
            ths = driver.find_elements(By.XPATH, '//tbody//tr/th')  # Finding all 'th' elements
            th_text.append([th.text for th in ths])  # Converting all 'th' elements to their text string
            th_text = [item for sublist in th_text for item in sublist]  # Flattening the list

            # Identifying the position of each relevant <th> element in the list
            meta_index = int(th_text.index("Reference"))
            add_index = int(th_text.index("Address"))
            prop_index = int(th_text.index("Proposal"))

            # Using the positions of the <th> elements to access the relevant
            # <td> elements on the page
            desc = driver.find_elements(By.XPATH, (f'//tbody//tr[{prop_index + 1}]//td'))
            address = driver.find_elements(By.XPATH, (f'//tbody//tr[{add_index + 1}]//td'))
            meta = driver.find_elements(By.XPATH, (f'//tbody//tr[{meta_index + 1}]//td'))

            # Extracting the text of each relevant <td> element
            description_text.append([d.get_attribute("innerHTML").strip() for d in desc])
            address_text.append([a.text for a in address])
            meta_text.append([m.text for m in meta])

            # If a 'Decision' row is on the page:
            if driver.find_elements(By.XPATH, '//th[contains(text(), "Decision")]'):
                try:
                    # Identify the position of the 'Decision' <th> element
                    deci_index = int(th_text.index("Decision"))
                    # Use this position to find the corresponding <td> element
                    decision = driver.find_elements(By.XPATH, (f'//tbody//tr[{deci_index + 1}]//td'))
                    # Extracting the text of the 'Decision' <td> element
                    decision_text.append([ds.text for ds in decision])
                except:
                    # If no 'Decision' is found:
                    decision_text.append(['No decision made'])
            break # Move on to the next date range if the above is successfully completed
        # If there are no applications on the page:
        elif driver.find_element(By.XPATH, '//div[@class="messagebox"]'):
            print('No results for this date range')
            break # Move on to the next date range
        else:
            break

# Flattening the lists of descriptions, addresses, metadata, and decisions
description_text = [item for sublist in description_text for item in sublist]
address_text = [item for sublist in address_text for item in sublist]
meta_text = [item for sublist in meta_text for item in sublist]
decision_text = [item for sublist in decision_text for item in sublist]

# Creating a dictionary to combine each list
dic = {'description': description_text, 'address': address_text, 'metadata': meta_text, 'decision': decision_text}

# Creating a dataframe from the dictionary
example_feb = pd.DataFrame.from_dict(dic, orient = 'index')

# Transposing the dataframe
example_feb = example_feb.transpose()

# Dropping duplicate metadata (application reference) values
example_feb = example_feb.drop_duplicates(subset = ["metadata"], keep = 'first')

# Saving the output to a CSV
example_feb.to_csv('example_feb.csv')

# Viewing the head of the dataframe and checking the length
display(example_feb.head(5))

print(f'Number of applications scraped in {url}: {len(example_feb)}')

# Closing the driver to prevent continual running
driver.quit()

